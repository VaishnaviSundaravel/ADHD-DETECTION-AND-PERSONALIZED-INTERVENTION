# -*- coding: utf-8 -*-
"""Miniproject30.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gkGxWxMEMTpXlG7vPqqFcqNNW4g73eSZ

STEP 1 — Import Libraries
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

from xgboost import XGBClassifier

"""STEP 2 — Load Dataset"""

from google.colab import files
files.upload()

import pandas as pd

df = pd.read_csv("adhd_data.csv")
df.head()

"""STEP 3 — Encode Categorical Variables"""

cat_cols = ['Gender', 'Educational_Level', 'Family_History']

df_encoded = df.copy()
for col in cat_cols:
    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col])

question_cols = [
    'Q1_1','Q1_2','Q1_3','Q1_4','Q1_5','Q1_6','Q1_7','Q1_8','Q1_9'
]

# Drop Q1 and Q2 questions
df_encoded = df_encoded.drop(question_cols, axis=1)

"""STEP 4 — Define Features (X) and Target (y)"""

X = df_encoded.drop('Diagnosis_Class', axis=1)
y = df_encoded['Diagnosis_Class']

"""STEP 5 — Split Train/Test Data"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

"""STEP 6 — Feature Scaling (Only for SVM)"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""STEP 7 — Train Machine Learning Models

7.1. Decision Tree
"""

dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)

"""7.2. Random Forest"""

rf_model = RandomForestClassifier(n_estimators=200, random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

"""7.3. SVM (Support Vector Classifier)"""

svm_model = SVC(kernel='rbf', probability=True, random_state=42)
svm_model.fit(X_train_scaled, y_train)
svm_pred = svm_model.predict(X_test_scaled)

"""7.4. XGBoost Classifier"""

xgb_model = XGBClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

xgb_model.fit(X_train, y_train)
xgb_pred = xgb_model.predict(X_test)

"""STEP 8 — Evaluate Each Model"""

def evaluate_model(name, y_test, y_pred):
    print(f"\nModel: {name}")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("\nClassification Report:\n", classification_report(y_test, y_pred))
    print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

evaluate_model("Decision Tree", y_test, dt_pred)
evaluate_model("Random Forest", y_test, rf_pred)
evaluate_model("SVM", y_test, svm_pred)
evaluate_model("XGBoost", y_test, xgb_pred)

"""STEP 9 — Compare Model Performance"""

model_scores = {
    "Decision Tree": accuracy_score(y_test, dt_pred),
    "Random Forest": accuracy_score(y_test, rf_pred),
    "SVM": accuracy_score(y_test, svm_pred),
    "XGBoost": accuracy_score(y_test, xgb_pred),
}

print(model_scores)

"""STEP 10 — Feature Importance"""

import matplotlib.pyplot as plt

feat_imp = pd.Series(rf_model.feature_importances_, index=X.columns)
feat_imp.nlargest(20).plot(kind='barh', figsize=(8, 10))
plt.title("Top Feature Importances (Random Forest)")
plt.show()

import matplotlib.pyplot as plt
from xgboost import plot_importance

plt.figure(figsize=(10, 12))
plot_importance(xgb_model, max_num_features=20)
plt.show()

"""STEP11 - Model performance charts"""

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

# Accuracy values
model_scores = {
    "Decision Tree": accuracy_score(y_test, dt_pred),
    "Random Forest": accuracy_score(y_test, rf_pred),
    "SVM": accuracy_score(y_test, svm_pred),
    "XGBoost": accuracy_score(y_test, xgb_pred),
}

# Extract names and scores
model_names = list(model_scores.keys())
accuracies = list(model_scores.values())

# Plotting
plt.figure(figsize=(8, 5))
plt.bar(model_names, accuracies)
plt.ylabel("Accuracy")
plt.ylim(0, 1)
plt.title("Model Accuracy Comparison")
plt.show()

"""STEP12 - SHAP Explainability

Install SHAP
"""

!pip install shap

"""SHAP for XGBoost"""

import shap

# Initialize explainer
explainer = shap.Explainer(xgb_model, X_test)
shap_values = explainer(X_test)

# ---- choose a sample and class ----
i = 0           # sample index to explain
class_id = 0    # class (0=No ADHD, 1=Inattentive, 2=Hyperactive, 3=Combined)

# ---- display force plot ----
shap.initjs()
shap.force_plot(
    explainer.expected_value[class_id],
    shap_values.values[i, :, class_id],
    X_test.iloc[i],
    matplotlib=False
)


# Summary plot (most important features)
shap.summary_plot(shap_values, X_test, plot_type="bar")

"""Detailed SHAP Interaction"""

shap.summary_plot(shap_values, X_test)

objective='multi:softprob'
num_class=4

print(xgb_model.get_params())